{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Basics of Neural Networks\n",
    "* <b>Learning Objective:</b> In the entrance exam, we asked you to implement a K-NN classifier to classify some tiny images extracted from CIFAR-10 dataset. Probably many of you noticed that the performances were quite bad. In this problem, you are going to implement a basic multi-layer fully connected neural network to perform the same classification task.\n",
    "* <b>Provided Code:</b> We provide the skeletons of classes you need to complete. Forward checking and gradient checkings are provided for verifying your implementation as well.\n",
    "* <b>TODOs:</b> You are asked to implement the forward passes and backward passes for standard layers and loss functions, various widely-used optimizers, and part of the training procedure. And finally we want you to train a network from scratch on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lib.fully_conn import *\n",
    "from lib.layer_utils import *\n",
    "from lib.grad_check import *\n",
    "from lib.datasets import *\n",
    "from lib.optim import *\n",
    "from lib.train import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data (CIFAR-10)\n",
    "Run the following code block to load in the properly splitted CIFAR-10 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: data_train Shape: (49000, 3, 32, 32)\n",
      "Name: data_val Shape: (1000, 3, 32, 32)\n",
      "Name: data_test Shape: (1000, 3, 32, 32)\n",
      "Name: labels_train Shape: (49000,)\n",
      "Name: labels_val Shape: (1000,)\n",
      "Name: labels_test Shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "data = CIFAR10_data()\n",
    "for k, v in data.iteritems():\n",
    "    print \"Name: {} Shape: {}\".format(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Standard Layers\n",
    "You will now implement all the following standard layers commonly seen in a fully connected neural network. Please refer to the file layer_utils.py under the directory lib. Take a look at each class skeleton, and we will walk you through the network layer by layer. We provide results of some examples we pre-computed for you for checking the forward pass, and also the gradient checking for the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC Forward\n",
    "In the class skeleton \"fc\", please complete the forward pass in function \"forward\", the input to the fc layer may not be of dimension (batch size, features size), it could be an image or any higher dimensional data. Make sure that you handle this dimensionality issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference:  2.48539291792e-09\n"
     ]
    }
   ],
   "source": [
    "# Test the fc forward function\n",
    "input_bz = 3\n",
    "input_dim = (6, 5, 4)\n",
    "output_dim = 4\n",
    "\n",
    "input_size = input_bz * np.prod(input_dim)\n",
    "weight_size = output_dim * np.prod(input_dim)\n",
    "\n",
    "single_fc = fc(np.prod(input_dim), output_dim, init_scale=0.02, name=\"fc_test\")\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(input_bz, *input_dim)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_dim), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "single_fc.params[single_fc.w_name] = w\n",
    "single_fc.params[single_fc.b_name] = b\n",
    "\n",
    "out = single_fc.forward(x)\n",
    "\n",
    "correct_out = np.array([[0.70157129, 0.83483484, 0.96809839, 1.10136194],\n",
    "                        [1.86723094, 2.02561647, 2.18400199, 2.34238752],\n",
    "                        [3.0328906,  3.2163981,  3.3999056,  3.5834131]])\n",
    "\n",
    "# Compare your output with the above pre-computed ones. \n",
    "# The difference should not be larger than 1e-8\n",
    "print \"Difference: \", rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC Backward\n",
    "Please complete the function \"backward\" as the backward pass of the fc layer. Follow the instructions in the comments to store gradients into the predefined dictionaries in the attributes of the class. Parameters of the layer are also stored in the predefined dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx Error:  9.86571682481e-09\n",
      "dw Error:  6.24641120993e-10\n",
      "db Error:  1.91615452972e-11\n"
     ]
    }
   ],
   "source": [
    "# Test the fc backward function\n",
    "x = np.random.randn(10, 2, 2, 3)\n",
    "w = np.random.randn(12, 10)\n",
    "b = np.random.randn(10)\n",
    "dout = np.random.randn(10, 10)\n",
    "\n",
    "single_fc = fc(np.prod(x.shape[1:]), 10, init_scale=5e-2, name=\"fc_test\")\n",
    "single_fc.params[single_fc.w_name] = w\n",
    "single_fc.params[single_fc.b_name] = b\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: single_fc.forward(x), x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: single_fc.forward(x), w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: single_fc.forward(x), b, dout)\n",
    "\n",
    "out = single_fc.forward(x)\n",
    "dx = single_fc.backward(dout)\n",
    "dw = single_fc.grads[single_fc.w_name]\n",
    "db = single_fc.grads[single_fc.b_name]\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print \"dx Error: \", rel_error(dx_num, dx)\n",
    "print \"dw Error: \", rel_error(dw_num, dw)\n",
    "print \"db Error: \", rel_error(db_num, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Forward\n",
    "In the class skeleton \"relu\", please complete the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference:  5.00000005012e-09\n"
     ]
    }
   ],
   "source": [
    "# Test the relu forward function\n",
    "x = np.linspace(-1.0, 1.0, num=12).reshape(3, 4)\n",
    "relu_f = relu(name=\"relu_f\")\n",
    "\n",
    "out = relu_f.forward(x)\n",
    "correct_out = np.array([[0.,          0.,        0.,         0.        ],\n",
    "                        [0.,          0.,        0.09090909, 0.27272727],\n",
    "                        [0.45454545, 0.63636364, 0.81818182, 1.        ]])\n",
    "\n",
    "# Compare your output with the above pre-computed ones. \n",
    "# The difference should not be larger than 1e-8\n",
    "print \"Difference: \", rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Backward\n",
    "Please complete the backward pass of the class relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx Error:  3.27561381025e-12\n"
     ]
    }
   ],
   "source": [
    "# Test the relu backward function\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "relu_b = relu(name=\"relu_b\")\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_b.forward(x), x, dout)\n",
    "\n",
    "out = relu_b.forward(x)\n",
    "dx = relu_b.backward(dout)\n",
    "\n",
    "# The error should not be larger than 1e-10\n",
    "print \"dx Error: \", rel_error(dx_num, dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Forward\n",
    "In the class \"dropout\", please complete the forward pass. Remember that the dropout is only applied during training phase, you should pay attention to this while implementing the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Dropout p =  0.25\n",
      "Mean of input:  5.00302284812\n",
      "Mean of output during training time:  5.08406115263\n",
      "Mean of output during testing time:  5.00302284812\n",
      "Fraction of output set to zero during training time:  0.7441\n",
      "Fraction of output set to zero during testing time:  0.0\n",
      "----------------------------------------------------------------\n",
      "Dropout p =  0.5\n",
      "Mean of input:  5.00302284812\n",
      "Mean of output during training time:  4.99249014509\n",
      "Mean of output during testing time:  5.00302284812\n",
      "Fraction of output set to zero during training time:  0.5027\n",
      "Fraction of output set to zero during testing time:  0.0\n",
      "----------------------------------------------------------------\n",
      "Dropout p =  0.75\n",
      "Mean of input:  5.00302284812\n",
      "Mean of output during training time:  5.00259641632\n",
      "Mean of output during testing time:  5.00302284812\n",
      "Fraction of output set to zero during training time:  0.2497\n",
      "Fraction of output set to zero during testing time:  0.0\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(100, 100) + 5.0\n",
    "\n",
    "print \"----------------------------------------------------------------\"\n",
    "for p in [0.25, 0.50, 0.75]:\n",
    "    dropout_f = dropout(p)\n",
    "    out = dropout_f.forward(x, True)\n",
    "    out_test = dropout_f.forward(x, False)\n",
    "\n",
    "    print \"Dropout p = \", p\n",
    "    print \"Mean of input: \", x.mean()\n",
    "    print \"Mean of output during training time: \", out.mean()\n",
    "    print \"Mean of output during testing time: \", out_test.mean()\n",
    "    print \"Fraction of output set to zero during training time: \", (out == 0).mean()\n",
    "    print \"Fraction of output set to zero during testing time: \", (out_test == 0).mean()\n",
    "    print \"----------------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Backward\n",
    "Please complete the backward pass. Again remember that the dropout is only applied during training phase, handle this in the backward pass as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx relative error:  3.00311891818e-11\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(5, 5) + 5\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "p = 0.75\n",
    "dropout_b = dropout(p, seed=100)\n",
    "out = dropout_b.forward(x, True)\n",
    "dx = dropout_b.backward(dout)\n",
    "dx_num = eval_numerical_gradient_array(lambda xx: dropout_b.forward(xx, True), x, dout)\n",
    "\n",
    "# The error should not be larger than 1e-9\n",
    "print 'dx relative error: ', rel_error(dx, dx_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing cascaded layers: FC + ReLU\n",
    "Please find the TestFCReLU function in fully_conn.py under lib directory. <br />\n",
    "You only need to complete few lines of code in the TODO block. <br />\n",
    "Please design an FC --> ReLU two-layer-mini-network where the parameters of them match the given x, w, and b <br />\n",
    "Please insert the corresponding names you defined for each layer to param_name_w, and param_name_b respectively. <br />\n",
    "Here you only modify the param_name part, the _w, and _b are automatically assigned during network setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  2.70712836868e-10\n",
      "dw error:  4.92451202602e-10\n",
      "db error:  3.27562531714e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(2, 3, 4)  # the input features\n",
    "w = np.random.randn(12, 10)   # the weight of fc layer\n",
    "b = np.random.randn(10)       # the bias of fc layer\n",
    "dout = np.random.randn(2, 10) # the gradients to the output, notice the shape\n",
    "\n",
    "tiny_net = TestFCReLU()\n",
    "\n",
    "tiny_net.net.assign(\"fc_layer_w\", w)\n",
    "tiny_net.net.assign(\"fc_layer_b\", b)\n",
    "\n",
    "out = tiny_net.forward(x)\n",
    "dx = tiny_net.backward(dout)\n",
    "\n",
    "dw = tiny_net.net.get_grads(\"fc_layer_w\")\n",
    "db = tiny_net.net.get_grads(\"fc_layer_b\")\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: tiny_net.forward(x), x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: tiny_net.forward(x), w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: tiny_net.forward(x), b, dout)\n",
    "\n",
    "# The errors should not be larger than 1e-7\n",
    "print \"dx error: \", rel_error(dx_num, dx)\n",
    "print \"dw error: \", rel_error(dw_num, dw)\n",
    "print \"db error: \", rel_error(db_num, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SoftMax Function and Loss Layer\n",
    "In the layer_utils.py, please first complete the function softmax, which will be use in the function cross_entropy. Please refer to the lecture slides of the mathematical expressions of the cross entropy loss function, and complete its forward pass and backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss:  1.60945846468\n",
      "dx error:  3.28599046685e-09\n"
     ]
    }
   ],
   "source": [
    "num_classes, num_inputs = 5, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "test_loss = cross_entropy()\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: test_loss.forward(x, y), x, verbose=False)\n",
    "\n",
    "loss = test_loss.forward(x, y)\n",
    "dx = test_loss.backward()\n",
    "\n",
    "# Test softmax_loss function. Loss should be around 1.609\n",
    "# and dx error should be at the scale of 1e-8 (or smaller)\n",
    "print \"Cross Entropy Loss: \", loss\n",
    "print \"dx error: \", rel_error(dx_num, dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a Small Fully Connected Network\n",
    "Please find the SmallFullyConnectedNetwork function in fully_conn.py under lib directory. <br />\n",
    "Again you only need to complete few lines of code in the TODO block. <br />\n",
    "Please design an FC --> ReLU --> FC --> ReLU network where the shapes of parameters match the given shapes <br />\n",
    "Please insert the corresponding names you defined for each layer to param_name_w, and param_name_b respectively. <br />\n",
    "Here you only modify the param_name part, the _w, and _b are automatically assigned during network setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing initialization ... \n",
      "Passed!\n",
      "Testing test-time forward pass ... \n",
      "Passed!\n",
      "Testing the loss ... Passed!\n",
      "Testing the gradients (error should be no larger than 1e-7) ...\n",
      "fc1_b relative error: 2.85e-09\n",
      "fc1_w relative error: 7.76e-09\n",
      "fc2_b relative error: 4.33e-07\n",
      "fc2_w relative error: 3.03e-09\n"
     ]
    }
   ],
   "source": [
    "model = SmallFullyConnectedNetwork()\n",
    "loss_func = cross_entropy()\n",
    "\n",
    "N, D, = 4, 4  # N: batch size, D: input dimension\n",
    "H, C  = 30, 7 # H: hidden dimension, C: output dimension\n",
    "std = 0.02\n",
    "x = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "print \"Testing initialization ... \"\n",
    "w1_std = abs(model.net.get_params(\"fc1_w\").std() - std)\n",
    "b1 = model.net.get_params(\"fc1_b\").std()\n",
    "w2_std = abs(model.net.get_params(\"fc2_w\").std() - std)\n",
    "b2 = model.net.get_params(\"fc2_b\").std()\n",
    "\n",
    "assert w1_std < std / 10, \"First layer weights do not seem right\"\n",
    "assert np.all(b1 == 0), \"First layer biases do not seem right\"\n",
    "assert w2_std < std / 10, \"Second layer weights do not seem right\"\n",
    "assert np.all(b2 == 0), \"Second layer biases do not seem right\"\n",
    "print \"Passed!\"\n",
    "\n",
    "print \"Testing test-time forward pass ... \"\n",
    "w1 = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "w2 = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "b1 = np.linspace(-0.1, 0.9, num=H)\n",
    "b2 = np.linspace(-0.9, 0.1, num=C)\n",
    "\n",
    "model.net.assign(\"fc1_w\", w1)\n",
    "model.net.assign(\"fc1_b\", b1)\n",
    "model.net.assign(\"fc2_w\", w2)\n",
    "model.net.assign(\"fc2_b\", b2)\n",
    "\n",
    "feats = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.forward(feats)\n",
    "correct_scores = np.asarray([[4.20670862, 4.87188359, 5.53705856, 6.20223352, 6.86740849, 7.53258346, 8.19775843],\n",
    "                             [4.74826036, 5.35984681, 5.97143326, 6.58301972, 7.19460617, 7.80619262, 8.41777907],\n",
    "                             [5.2898121,  5.84781003, 6.40580797, 6.96380591, 7.52180384, 8.07980178, 8.63779971],\n",
    "                             [5.83136384, 6.33577326, 6.84018268, 7.3445921,  7.84900151, 8.35341093, 8.85782035]])\n",
    "scores_diff = np.sum(np.abs(scores - correct_scores))\n",
    "assert scores_diff < 1e-6, \"Your implementation might went wrong!\"\n",
    "print \"Passed!\"\n",
    "\n",
    "print \"Testing the loss ...\",\n",
    "y = np.asarray([0, 5, 1, 4])\n",
    "loss = loss_func.forward(scores, y)\n",
    "dLoss = loss_func.backward()\n",
    "correct_loss = 2.90181552716\n",
    "assert abs(loss - correct_loss) < 1e-10, \"Your implementation might went wrong!\"\n",
    "print \"Passed!\"\n",
    "\n",
    "print \"Testing the gradients (error should be no larger than 1e-7) ...\"\n",
    "din = model.backward(dLoss)\n",
    "for layer in model.net.layers:\n",
    "    if not layer.params:\n",
    "        continue\n",
    "    for name in sorted(layer.grads):\n",
    "        f = lambda _: loss_func.forward(model.forward(feats), y)\n",
    "        grad_num = eval_numerical_gradient(f, layer.params[name], verbose=False)\n",
    "        print '%s relative error: %.2e' % (name, rel_error(grad_num, layer.grads[name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a Fully Connected Network regularized with Dropout\n",
    "Please find the DropoutNet function in fully_conn.py under lib directory. <br />\n",
    "For this part you don't need to design a new network, just simply run the following test code <br />\n",
    "If something goes wrong, you might want to double check your dropout implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout p = 0.0\n",
      "Loss (should be ~2.30) :  2.30285163514\n",
      "Error of gradients should be no larger than 1e-5\n",
      "fc1_b relative error: 3.86706744278e-08\n",
      "fc1_w relative error: 1.68428083667e-06\n",
      "fc2_b relative error: 3.81209866093e-09\n",
      "fc2_w relative error: 2.26785963522e-06\n",
      "fc3_b relative error: 1.52481732925e-10\n",
      "fc3_w relative error: 1.16962221259e-07\n",
      "\n",
      "Dropout p = 0.25\n",
      "Loss (should be ~2.30) :  2.30263148624\n",
      "Error of gradients should be no larger than 1e-5\n",
      "fc1_b relative error: 3.64409537483e-08\n",
      "fc1_w relative error: 2.63537430505e-06\n",
      "fc2_b relative error: 1.53162175166e-08\n",
      "fc2_w relative error: 1.69797594532e-06\n",
      "fc3_b relative error: 1.65686333953e-10\n",
      "fc3_w relative error: 5.22081466852e-09\n",
      "\n",
      "Dropout p = 0.5\n",
      "Loss (should be ~2.30) :  2.30443780379\n",
      "Error of gradients should be no larger than 1e-5\n",
      "fc1_b relative error: 2.2211808585e-08\n",
      "fc1_w relative error: 7.91512427714e-07\n",
      "fc2_b relative error: 9.43469441119e-07\n",
      "fc2_w relative error: 4.22495438917e-05\n",
      "fc3_b relative error: 8.85507369309e-11\n",
      "fc3_w relative error: 2.83107528665e-07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N, D, C = 3, 15, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "seed = 123\n",
    "\n",
    "for dropout_p in [0., 0.25, 0.5]:\n",
    "    print \"Dropout p =\", dropout_p\n",
    "    model = DropoutNet(dropout_p=dropout_p, seed=seed)\n",
    "    loss_func = cross_entropy()\n",
    "    output = model.forward(X, True)\n",
    "    loss = loss_func.forward(output, y)\n",
    "    dLoss = loss_func.backward()\n",
    "    dX = model.backward(dLoss)\n",
    "    grads = model.net.grads\n",
    "    print \"Loss (should be ~2.30) : \", loss\n",
    "\n",
    "    print \"Error of gradients should be no larger than 1e-5\"\n",
    "    for name in sorted(model.net.params):\n",
    "        f = lambda _: loss_func.forward(model.forward(X, True), y)\n",
    "        grad_num = eval_numerical_gradient(f, model.net.params[name], verbose=False, h=1e-5)\n",
    "        print \"{} relative error: {}\".format(name, rel_error(grad_num, grads[name]))\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Network\n",
    "In this section, we defined a TinyNet class for you to fill in the TODO block in fully_conn.py.\n",
    "* Here please design a two layer fully connected network for this part.\n",
    "* Please read the train.py under lib directory carefully and complete the TODO blocks in the train_net function first.\n",
    "* In addition, read how the SGD function is implemented in optim.py, you will be asked to complete three other optimization methods in the later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Arrange the data\n",
    "data_dict = {\n",
    "    \"data_train\": (data[\"data_train\"], data[\"labels_train\"]),\n",
    "    \"data_val\": (data[\"data_val\"], data[\"labels_val\"]),\n",
    "    \"data_test\": (data[\"data_test\"], data[\"labels_test\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = TinyNet()\n",
    "loss_f = cross_entropy()\n",
    "optimizer = SGD(model.net, 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now train the network to achieve at least 50% validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 3820) loss: 2.30736643768\n",
      "(Iteration 11 / 3820) loss: 2.28155912618\n",
      "(Iteration 21 / 3820) loss: 2.23506470241\n",
      "(Iteration 31 / 3820) loss: 2.19054798372\n",
      "(Iteration 41 / 3820) loss: 2.15292820387\n",
      "(Iteration 51 / 3820) loss: 2.12146140798\n",
      "(Iteration 61 / 3820) loss: 2.09549250113\n",
      "(Iteration 71 / 3820) loss: 2.07353255695\n",
      "(Iteration 81 / 3820) loss: 2.05433245344\n",
      "(Iteration 91 / 3820) loss: 2.03707296113\n",
      "(Iteration 101 / 3820) loss: 2.02116866522\n",
      "(Iteration 111 / 3820) loss: 2.00610643226\n",
      "(Iteration 121 / 3820) loss: 1.99125513371\n",
      "(Iteration 131 / 3820) loss: 1.97573384263\n",
      "(Iteration 141 / 3820) loss: 1.96026719284\n",
      "(Iteration 151 / 3820) loss: 1.94543133083\n",
      "(Iteration 161 / 3820) loss: 1.9312711656\n",
      "(Iteration 171 / 3820) loss: 1.91805779555\n",
      "(Iteration 181 / 3820) loss: 1.90595536716\n",
      "(Iteration 191 / 3820) loss: 1.89484244427\n",
      "(Iteration 201 / 3820) loss: 1.88454371182\n",
      "(Iteration 211 / 3820) loss: 1.87493036833\n",
      "(Iteration 221 / 3820) loss: 1.86591826071\n",
      "(Iteration 231 / 3820) loss: 1.85742057541\n",
      "(Iteration 241 / 3820) loss: 1.84938422495\n",
      "(Iteration 251 / 3820) loss: 1.8417571812\n",
      "(Iteration 261 / 3820) loss: 1.834485819\n",
      "(Iteration 271 / 3820) loss: 1.82755810497\n",
      "(Iteration 281 / 3820) loss: 1.82093588977\n",
      "(Iteration 291 / 3820) loss: 1.81458752673\n",
      "(Iteration 301 / 3820) loss: 1.80846823498\n",
      "(Iteration 311 / 3820) loss: 1.80252933232\n",
      "(Iteration 321 / 3820) loss: 1.79672370164\n",
      "(Iteration 331 / 3820) loss: 1.79089000099\n",
      "(Iteration 341 / 3820) loss: 1.78501445041\n",
      "(Iteration 351 / 3820) loss: 1.77917586353\n",
      "(Iteration 361 / 3820) loss: 1.77352907328\n",
      "(Iteration 371 / 3820) loss: 1.768189772\n",
      "(Iteration 381 / 3820) loss: 1.76311782795\n",
      "(Epoch 1 / 10) Training Accuracy: 0.402244897959, Validation Accuracy: 0.406\n",
      "(Iteration 391 / 3820) loss: 1.75825219585\n",
      "(Iteration 401 / 3820) loss: 1.75356215212\n",
      "(Iteration 411 / 3820) loss: 1.74904263388\n",
      "(Iteration 421 / 3820) loss: 1.74467029141\n",
      "(Iteration 431 / 3820) loss: 1.74043390491\n",
      "(Iteration 441 / 3820) loss: 1.73631700458\n",
      "(Iteration 451 / 3820) loss: 1.73231236839\n",
      "(Iteration 461 / 3820) loss: 1.72841436462\n",
      "(Iteration 471 / 3820) loss: 1.72461815191\n",
      "(Iteration 481 / 3820) loss: 1.72091126992\n",
      "(Iteration 491 / 3820) loss: 1.7172939527\n",
      "(Iteration 501 / 3820) loss: 1.7137615336\n",
      "(Iteration 511 / 3820) loss: 1.71030983123\n",
      "(Iteration 521 / 3820) loss: 1.70694027227\n",
      "(Iteration 531 / 3820) loss: 1.70364524687\n",
      "(Iteration 541 / 3820) loss: 1.70041421527\n",
      "(Iteration 551 / 3820) loss: 1.69724601095\n",
      "(Iteration 561 / 3820) loss: 1.69414620336\n",
      "(Iteration 571 / 3820) loss: 1.69110557421\n",
      "(Iteration 581 / 3820) loss: 1.6881181218\n",
      "(Iteration 591 / 3820) loss: 1.68518111836\n",
      "(Iteration 601 / 3820) loss: 1.68229305072\n",
      "(Iteration 611 / 3820) loss: 1.67944937886\n",
      "(Iteration 621 / 3820) loss: 1.67663811488\n",
      "(Iteration 631 / 3820) loss: 1.67386438549\n",
      "(Iteration 641 / 3820) loss: 1.67113065013\n",
      "(Iteration 651 / 3820) loss: 1.66844327797\n",
      "(Iteration 661 / 3820) loss: 1.66579565348\n",
      "(Iteration 671 / 3820) loss: 1.66318603979\n",
      "(Iteration 681 / 3820) loss: 1.66061831207\n",
      "(Iteration 691 / 3820) loss: 1.65808699484\n",
      "(Iteration 701 / 3820) loss: 1.65559770629\n",
      "(Iteration 711 / 3820) loss: 1.65313992635\n",
      "(Iteration 721 / 3820) loss: 1.65071504022\n",
      "(Iteration 731 / 3820) loss: 1.64832803978\n",
      "(Iteration 741 / 3820) loss: 1.64597536224\n",
      "(Iteration 751 / 3820) loss: 1.64365580653\n",
      "(Iteration 761 / 3820) loss: 1.64136646282\n",
      "(Epoch 2 / 10) Training Accuracy: 0.440489795918, Validation Accuracy: 0.434\n",
      "(Iteration 771 / 3820) loss: 1.63910901522\n",
      "(Iteration 781 / 3820) loss: 1.63688426743\n",
      "(Iteration 791 / 3820) loss: 1.63469504982\n",
      "(Iteration 801 / 3820) loss: 1.63253184017\n",
      "(Iteration 811 / 3820) loss: 1.63040013964\n",
      "(Iteration 821 / 3820) loss: 1.62829801013\n",
      "(Iteration 831 / 3820) loss: 1.62622378079\n",
      "(Iteration 841 / 3820) loss: 1.62417637972\n",
      "(Iteration 851 / 3820) loss: 1.62215643525\n",
      "(Iteration 861 / 3820) loss: 1.62016143675\n",
      "(Iteration 871 / 3820) loss: 1.61819278948\n",
      "(Iteration 881 / 3820) loss: 1.61624905527\n",
      "(Iteration 891 / 3820) loss: 1.61432722775\n",
      "(Iteration 901 / 3820) loss: 1.61242825032\n",
      "(Iteration 911 / 3820) loss: 1.61055337323\n",
      "(Iteration 921 / 3820) loss: 1.60870235633\n",
      "(Iteration 931 / 3820) loss: 1.60687710456\n",
      "(Iteration 941 / 3820) loss: 1.60507540952\n",
      "(Iteration 951 / 3820) loss: 1.60329663287\n",
      "(Iteration 961 / 3820) loss: 1.60154060374\n",
      "(Iteration 971 / 3820) loss: 1.59980436545\n",
      "(Iteration 981 / 3820) loss: 1.5980872374\n",
      "(Iteration 991 / 3820) loss: 1.59638964969\n",
      "(Iteration 1001 / 3820) loss: 1.59470976375\n",
      "(Iteration 1011 / 3820) loss: 1.59304811156\n",
      "(Iteration 1021 / 3820) loss: 1.59140390155\n",
      "(Iteration 1031 / 3820) loss: 1.58977823175\n",
      "(Iteration 1041 / 3820) loss: 1.58817087672\n",
      "(Iteration 1051 / 3820) loss: 1.58658221765\n",
      "(Iteration 1061 / 3820) loss: 1.58501038019\n",
      "(Iteration 1071 / 3820) loss: 1.58345567977\n",
      "(Iteration 1081 / 3820) loss: 1.58191774471\n",
      "(Iteration 1091 / 3820) loss: 1.58039548358\n",
      "(Iteration 1101 / 3820) loss: 1.57888798571\n",
      "(Iteration 1111 / 3820) loss: 1.57739445908\n",
      "(Iteration 1121 / 3820) loss: 1.57591390633\n",
      "(Iteration 1131 / 3820) loss: 1.57444648557\n",
      "(Iteration 1141 / 3820) loss: 1.57299097006\n",
      "(Epoch 3 / 10) Training Accuracy: 0.462857142857, Validation Accuracy: 0.457\n",
      "(Iteration 1151 / 3820) loss: 1.57154889921\n",
      "(Iteration 1161 / 3820) loss: 1.57012108217\n",
      "(Iteration 1171 / 3820) loss: 1.5687057502\n",
      "(Iteration 1181 / 3820) loss: 1.5673022439\n",
      "(Iteration 1191 / 3820) loss: 1.56591147872\n",
      "(Iteration 1201 / 3820) loss: 1.56453289921\n",
      "(Iteration 1211 / 3820) loss: 1.56316486984\n",
      "(Iteration 1221 / 3820) loss: 1.56180738704\n",
      "(Iteration 1231 / 3820) loss: 1.56046080324\n",
      "(Iteration 1241 / 3820) loss: 1.55912334692\n",
      "(Iteration 1251 / 3820) loss: 1.55779517272\n",
      "(Iteration 1261 / 3820) loss: 1.55647621795\n",
      "(Iteration 1271 / 3820) loss: 1.55516500174\n",
      "(Iteration 1281 / 3820) loss: 1.55386226143\n",
      "(Iteration 1291 / 3820) loss: 1.55256819464\n",
      "(Iteration 1301 / 3820) loss: 1.55128282888\n",
      "(Iteration 1311 / 3820) loss: 1.55000638128\n",
      "(Iteration 1321 / 3820) loss: 1.54873964349\n",
      "(Iteration 1331 / 3820) loss: 1.54748164802\n",
      "(Iteration 1341 / 3820) loss: 1.5462307949\n",
      "(Iteration 1351 / 3820) loss: 1.54498720718\n",
      "(Iteration 1361 / 3820) loss: 1.543751812\n",
      "(Iteration 1371 / 3820) loss: 1.54252373561\n",
      "(Iteration 1381 / 3820) loss: 1.54130382934\n",
      "(Iteration 1391 / 3820) loss: 1.54009170172\n",
      "(Iteration 1401 / 3820) loss: 1.53888738862\n",
      "(Iteration 1411 / 3820) loss: 1.53769116511\n",
      "(Iteration 1421 / 3820) loss: 1.53650221961\n",
      "(Iteration 1431 / 3820) loss: 1.53532101561\n",
      "(Iteration 1441 / 3820) loss: 1.53414595698\n",
      "(Iteration 1451 / 3820) loss: 1.53297749257\n",
      "(Iteration 1461 / 3820) loss: 1.53181688083\n",
      "(Iteration 1471 / 3820) loss: 1.53066274938\n",
      "(Iteration 1481 / 3820) loss: 1.52951436724\n",
      "(Iteration 1491 / 3820) loss: 1.52837331814\n",
      "(Iteration 1501 / 3820) loss: 1.52723884689\n",
      "(Iteration 1511 / 3820) loss: 1.52610978418\n",
      "(Iteration 1521 / 3820) loss: 1.52498676771\n",
      "(Epoch 4 / 10) Training Accuracy: 0.479346938776, Validation Accuracy: 0.464\n",
      "(Iteration 1531 / 3820) loss: 1.52386846282\n",
      "(Iteration 1541 / 3820) loss: 1.52275491302\n",
      "(Iteration 1551 / 3820) loss: 1.52164688704\n",
      "(Iteration 1561 / 3820) loss: 1.52054343139\n",
      "(Iteration 1571 / 3820) loss: 1.51944442306\n",
      "(Iteration 1581 / 3820) loss: 1.5183513462\n",
      "(Iteration 1591 / 3820) loss: 1.51726346213\n",
      "(Iteration 1601 / 3820) loss: 1.51618126387\n",
      "(Iteration 1611 / 3820) loss: 1.51510434375\n",
      "(Iteration 1621 / 3820) loss: 1.51403241621\n",
      "(Iteration 1631 / 3820) loss: 1.51296491029\n",
      "(Iteration 1641 / 3820) loss: 1.51190317929\n",
      "(Iteration 1651 / 3820) loss: 1.51084696269\n",
      "(Iteration 1661 / 3820) loss: 1.50979569347\n",
      "(Iteration 1671 / 3820) loss: 1.50874897349\n",
      "(Iteration 1681 / 3820) loss: 1.50770614069\n",
      "(Iteration 1691 / 3820) loss: 1.50666810609\n",
      "(Iteration 1701 / 3820) loss: 1.50563274154\n",
      "(Iteration 1711 / 3820) loss: 1.50460026228\n",
      "(Iteration 1721 / 3820) loss: 1.5035714561\n",
      "(Iteration 1731 / 3820) loss: 1.50254620414\n",
      "(Iteration 1741 / 3820) loss: 1.50152466853\n",
      "(Iteration 1751 / 3820) loss: 1.50050662199\n",
      "(Iteration 1761 / 3820) loss: 1.49949353078\n",
      "(Iteration 1771 / 3820) loss: 1.49848507376\n",
      "(Iteration 1781 / 3820) loss: 1.49748029746\n",
      "(Iteration 1791 / 3820) loss: 1.49647842553\n",
      "(Iteration 1801 / 3820) loss: 1.49547850924\n",
      "(Iteration 1811 / 3820) loss: 1.49448132004\n",
      "(Iteration 1821 / 3820) loss: 1.49348794677\n",
      "(Iteration 1831 / 3820) loss: 1.49249853163\n",
      "(Iteration 1841 / 3820) loss: 1.49151328254\n",
      "(Iteration 1851 / 3820) loss: 1.49053201271\n",
      "(Iteration 1861 / 3820) loss: 1.48955293877\n",
      "(Iteration 1871 / 3820) loss: 1.48857587263\n",
      "(Iteration 1881 / 3820) loss: 1.48760112957\n",
      "(Iteration 1891 / 3820) loss: 1.486627186\n",
      "(Iteration 1901 / 3820) loss: 1.48565420725\n",
      "(Epoch 5 / 10) Training Accuracy: 0.493, Validation Accuracy: 0.471\n",
      "(Iteration 1911 / 3820) loss: 1.48468179843\n",
      "(Iteration 1921 / 3820) loss: 1.48371176462\n",
      "(Iteration 1931 / 3820) loss: 1.48274211775\n",
      "(Iteration 1941 / 3820) loss: 1.48177642111\n",
      "(Iteration 1951 / 3820) loss: 1.48081290045\n",
      "(Iteration 1961 / 3820) loss: 1.47985216331\n",
      "(Iteration 1971 / 3820) loss: 1.47889331845\n",
      "(Iteration 1981 / 3820) loss: 1.47793679227\n",
      "(Iteration 1991 / 3820) loss: 1.47698394582\n",
      "(Iteration 2001 / 3820) loss: 1.4760352287\n",
      "(Iteration 2011 / 3820) loss: 1.47509101598\n",
      "(Iteration 2021 / 3820) loss: 1.47415120267\n",
      "(Iteration 2031 / 3820) loss: 1.47321460221\n",
      "(Iteration 2041 / 3820) loss: 1.47228132389\n",
      "(Iteration 2051 / 3820) loss: 1.47135207485\n",
      "(Iteration 2061 / 3820) loss: 1.47042588414\n",
      "(Iteration 2071 / 3820) loss: 1.46950290651\n",
      "(Iteration 2081 / 3820) loss: 1.46858383972\n",
      "(Iteration 2091 / 3820) loss: 1.46766812905\n",
      "(Iteration 2101 / 3820) loss: 1.46675527789\n",
      "(Iteration 2111 / 3820) loss: 1.46584525639\n",
      "(Iteration 2121 / 3820) loss: 1.4649376535\n",
      "(Iteration 2131 / 3820) loss: 1.46403240097\n",
      "(Iteration 2141 / 3820) loss: 1.46313031664\n",
      "(Iteration 2151 / 3820) loss: 1.46223182114\n",
      "(Iteration 2161 / 3820) loss: 1.46133570544\n",
      "(Iteration 2171 / 3820) loss: 1.46044283244\n",
      "(Iteration 2181 / 3820) loss: 1.45955330863\n",
      "(Iteration 2191 / 3820) loss: 1.4586667751\n",
      "(Iteration 2201 / 3820) loss: 1.45778275893\n",
      "(Iteration 2211 / 3820) loss: 1.45690167665\n",
      "(Iteration 2221 / 3820) loss: 1.45602386215\n",
      "(Iteration 2231 / 3820) loss: 1.45514894541\n",
      "(Iteration 2241 / 3820) loss: 1.45427716569\n",
      "(Iteration 2251 / 3820) loss: 1.45340817382\n",
      "(Iteration 2261 / 3820) loss: 1.45254218597\n",
      "(Iteration 2271 / 3820) loss: 1.45167872585\n",
      "(Iteration 2281 / 3820) loss: 1.45081750112\n",
      "(Iteration 2291 / 3820) loss: 1.44995904259\n",
      "(Epoch 6 / 10) Training Accuracy: 0.505489795918, Validation Accuracy: 0.48\n",
      "(Iteration 2301 / 3820) loss: 1.44910325071\n",
      "(Iteration 2311 / 3820) loss: 1.44825002909\n",
      "(Iteration 2321 / 3820) loss: 1.44739959192\n",
      "(Iteration 2331 / 3820) loss: 1.44655117155\n",
      "(Iteration 2341 / 3820) loss: 1.44570442938\n",
      "(Iteration 2351 / 3820) loss: 1.44486019078\n",
      "(Iteration 2361 / 3820) loss: 1.44401811145\n",
      "(Iteration 2371 / 3820) loss: 1.44317857292\n",
      "(Iteration 2381 / 3820) loss: 1.44234197833\n",
      "(Iteration 2391 / 3820) loss: 1.44150750353\n",
      "(Iteration 2401 / 3820) loss: 1.44067574068\n",
      "(Iteration 2411 / 3820) loss: 1.43984602787\n",
      "(Iteration 2421 / 3820) loss: 1.43901834982\n",
      "(Iteration 2431 / 3820) loss: 1.43819325402\n",
      "(Iteration 2441 / 3820) loss: 1.43736940827\n",
      "(Iteration 2451 / 3820) loss: 1.43654761928\n",
      "(Iteration 2461 / 3820) loss: 1.43572753414\n",
      "(Iteration 2471 / 3820) loss: 1.43490904224\n",
      "(Iteration 2481 / 3820) loss: 1.43409229358\n",
      "(Iteration 2491 / 3820) loss: 1.43327730803\n",
      "(Iteration 2501 / 3820) loss: 1.43246431811\n",
      "(Iteration 2511 / 3820) loss: 1.43165372915\n",
      "(Iteration 2521 / 3820) loss: 1.4308456643\n",
      "(Iteration 2531 / 3820) loss: 1.43004019286\n",
      "(Iteration 2541 / 3820) loss: 1.42923654167\n",
      "(Iteration 2551 / 3820) loss: 1.42843409688\n",
      "(Iteration 2561 / 3820) loss: 1.42763294551\n",
      "(Iteration 2571 / 3820) loss: 1.42683428407\n",
      "(Iteration 2581 / 3820) loss: 1.42603745683\n",
      "(Iteration 2591 / 3820) loss: 1.42524287246\n",
      "(Iteration 2601 / 3820) loss: 1.42445023029\n",
      "(Iteration 2611 / 3820) loss: 1.42365937604\n",
      "(Iteration 2621 / 3820) loss: 1.42287046295\n",
      "(Iteration 2631 / 3820) loss: 1.42208336478\n",
      "(Iteration 2641 / 3820) loss: 1.42129755646\n",
      "(Iteration 2651 / 3820) loss: 1.42051380299\n",
      "(Iteration 2661 / 3820) loss: 1.419732503\n",
      "(Iteration 2671 / 3820) loss: 1.41895284674\n",
      "(Epoch 7 / 10) Training Accuracy: 0.518163265306, Validation Accuracy: 0.496\n",
      "(Iteration 2681 / 3820) loss: 1.41817427704\n",
      "(Iteration 2691 / 3820) loss: 1.41739712366\n",
      "(Iteration 2701 / 3820) loss: 1.41662172294\n",
      "(Iteration 2711 / 3820) loss: 1.41584804888\n",
      "(Iteration 2721 / 3820) loss: 1.41507648208\n",
      "(Iteration 2731 / 3820) loss: 1.41430638497\n",
      "(Iteration 2741 / 3820) loss: 1.41353806467\n",
      "(Iteration 2751 / 3820) loss: 1.41277186634\n",
      "(Iteration 2761 / 3820) loss: 1.4120075122\n",
      "(Iteration 2771 / 3820) loss: 1.41124483309\n",
      "(Iteration 2781 / 3820) loss: 1.41048353743\n",
      "(Iteration 2791 / 3820) loss: 1.40972378376\n",
      "(Iteration 2801 / 3820) loss: 1.40896548135\n",
      "(Iteration 2811 / 3820) loss: 1.40820797618\n",
      "(Iteration 2821 / 3820) loss: 1.40745133412\n",
      "(Iteration 2831 / 3820) loss: 1.40669601549\n",
      "(Iteration 2841 / 3820) loss: 1.40594194942\n",
      "(Iteration 2851 / 3820) loss: 1.4051897696\n",
      "(Iteration 2861 / 3820) loss: 1.40443918593\n",
      "(Iteration 2871 / 3820) loss: 1.40369063896\n",
      "(Iteration 2881 / 3820) loss: 1.40294326971\n",
      "(Iteration 2891 / 3820) loss: 1.40219773512\n",
      "(Iteration 2901 / 3820) loss: 1.40145372065\n",
      "(Iteration 2911 / 3820) loss: 1.40071116195\n",
      "(Iteration 2921 / 3820) loss: 1.39997047477\n",
      "(Iteration 2931 / 3820) loss: 1.39923142201\n",
      "(Iteration 2941 / 3820) loss: 1.39849359619\n",
      "(Iteration 2951 / 3820) loss: 1.39775736643\n",
      "(Iteration 2961 / 3820) loss: 1.39702263644\n",
      "(Iteration 2971 / 3820) loss: 1.39628939505\n",
      "(Iteration 2981 / 3820) loss: 1.39555745879\n",
      "(Iteration 2991 / 3820) loss: 1.39482635681\n",
      "(Iteration 3001 / 3820) loss: 1.39409694403\n",
      "(Iteration 3011 / 3820) loss: 1.39336894707\n",
      "(Iteration 3021 / 3820) loss: 1.39264244993\n",
      "(Iteration 3031 / 3820) loss: 1.39191721032\n",
      "(Iteration 3041 / 3820) loss: 1.39119332816\n",
      "(Iteration 3051 / 3820) loss: 1.39047080189\n",
      "(Epoch 8 / 10) Training Accuracy: 0.528489795918, Validation Accuracy: 0.498\n",
      "(Iteration 3061 / 3820) loss: 1.38974963386\n",
      "(Iteration 3071 / 3820) loss: 1.38902998815\n",
      "(Iteration 3081 / 3820) loss: 1.38831135846\n",
      "(Iteration 3091 / 3820) loss: 1.38759378339\n",
      "(Iteration 3101 / 3820) loss: 1.38687720678\n",
      "(Iteration 3111 / 3820) loss: 1.38616198272\n",
      "(Iteration 3121 / 3820) loss: 1.38544729114\n",
      "(Iteration 3131 / 3820) loss: 1.38473417323\n",
      "(Iteration 3141 / 3820) loss: 1.38402264095\n",
      "(Iteration 3151 / 3820) loss: 1.38331278427\n",
      "(Iteration 3161 / 3820) loss: 1.38260421744\n",
      "(Iteration 3171 / 3820) loss: 1.38189700166\n",
      "(Iteration 3181 / 3820) loss: 1.38119094738\n",
      "(Iteration 3191 / 3820) loss: 1.38048663576\n",
      "(Iteration 3201 / 3820) loss: 1.37978373367\n",
      "(Iteration 3211 / 3820) loss: 1.37908228438\n",
      "(Iteration 3221 / 3820) loss: 1.37838201039\n",
      "(Iteration 3231 / 3820) loss: 1.37768282121\n",
      "(Iteration 3241 / 3820) loss: 1.37698509662\n",
      "(Iteration 3251 / 3820) loss: 1.37628905654\n",
      "(Iteration 3261 / 3820) loss: 1.37559441377\n",
      "(Iteration 3271 / 3820) loss: 1.37490095659\n",
      "(Iteration 3281 / 3820) loss: 1.37420846016\n",
      "(Iteration 3291 / 3820) loss: 1.37351699121\n",
      "(Iteration 3301 / 3820) loss: 1.37282658644\n",
      "(Iteration 3311 / 3820) loss: 1.37213745132\n",
      "(Iteration 3321 / 3820) loss: 1.3714491038\n",
      "(Iteration 3331 / 3820) loss: 1.37076149436\n",
      "(Iteration 3341 / 3820) loss: 1.37007510988\n",
      "(Iteration 3351 / 3820) loss: 1.3693897249\n",
      "(Iteration 3361 / 3820) loss: 1.36870572999\n",
      "(Iteration 3371 / 3820) loss: 1.36802305699\n",
      "(Iteration 3381 / 3820) loss: 1.36734134756\n",
      "(Iteration 3391 / 3820) loss: 1.3666610371\n",
      "(Iteration 3401 / 3820) loss: 1.36598216558\n",
      "(Iteration 3411 / 3820) loss: 1.36530464977\n",
      "(Iteration 3421 / 3820) loss: 1.36462848673\n",
      "(Iteration 3431 / 3820) loss: 1.36395357748\n",
      "(Epoch 9 / 10) Training Accuracy: 0.538183673469, Validation Accuracy: 0.503\n",
      "(Iteration 3441 / 3820) loss: 1.36327981214\n",
      "(Iteration 3451 / 3820) loss: 1.36260752693\n",
      "(Iteration 3461 / 3820) loss: 1.36193653515\n",
      "(Iteration 3471 / 3820) loss: 1.36126639844\n",
      "(Iteration 3481 / 3820) loss: 1.36059710121\n",
      "(Iteration 3491 / 3820) loss: 1.35992908133\n",
      "(Iteration 3501 / 3820) loss: 1.35926252151\n",
      "(Iteration 3511 / 3820) loss: 1.3585972539\n",
      "(Iteration 3521 / 3820) loss: 1.35793328465\n",
      "(Iteration 3531 / 3820) loss: 1.35727060326\n",
      "(Iteration 3541 / 3820) loss: 1.35660862105\n",
      "(Iteration 3551 / 3820) loss: 1.35594775925\n",
      "(Iteration 3561 / 3820) loss: 1.35528788102\n",
      "(Iteration 3571 / 3820) loss: 1.3546288621\n",
      "(Iteration 3581 / 3820) loss: 1.353970378\n",
      "(Iteration 3591 / 3820) loss: 1.35331275081\n",
      "(Iteration 3601 / 3820) loss: 1.35265675386\n",
      "(Iteration 3611 / 3820) loss: 1.35200264803\n",
      "(Iteration 3621 / 3820) loss: 1.35134995999\n",
      "(Iteration 3631 / 3820) loss: 1.35069836976\n",
      "(Iteration 3641 / 3820) loss: 1.35004814639\n",
      "(Iteration 3651 / 3820) loss: 1.34939913377\n",
      "(Iteration 3661 / 3820) loss: 1.34875081933\n",
      "(Iteration 3671 / 3820) loss: 1.34810351971\n",
      "(Iteration 3681 / 3820) loss: 1.3474571152\n",
      "(Iteration 3691 / 3820) loss: 1.34681200251\n",
      "(Iteration 3701 / 3820) loss: 1.34616793064\n",
      "(Iteration 3711 / 3820) loss: 1.34552480488\n",
      "(Iteration 3721 / 3820) loss: 1.34488264724\n",
      "(Iteration 3731 / 3820) loss: 1.34424130527\n",
      "(Iteration 3741 / 3820) loss: 1.34360075983\n",
      "(Iteration 3751 / 3820) loss: 1.34296110628\n",
      "(Iteration 3761 / 3820) loss: 1.34232256335\n",
      "(Iteration 3771 / 3820) loss: 1.34168528732\n",
      "(Iteration 3781 / 3820) loss: 1.34104865761\n",
      "(Iteration 3791 / 3820) loss: 1.34041299314\n",
      "(Iteration 3801 / 3820) loss: 1.33977831116\n",
      "(Iteration 3811 / 3820) loss: 1.33914466229\n",
      "(Epoch 10 / 10) Training Accuracy: 0.548183673469, Validation Accuracy: 0.509\n"
     ]
    }
   ],
   "source": [
    "results = None\n",
    "#############################################################################\n",
    "# TODO: Use the train_net function you completed to train a network         #\n",
    "#############################################################################\n",
    "results = train_net(data_dict, model, loss_f, optimizer, 128, 10, verbose=True)\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "opt_params, loss_hist, train_acc_hist, val_acc_hist = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take a look at what names of params were stored\n",
    "print opt_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Demo: How to load the parameters to a newly defined network\n",
    "model = TinyNet()\n",
    "model.net.load(opt_params)\n",
    "val_acc = compute_acc(model, data[\"data_val\"], data[\"labels_val\"])\n",
    "print \"Validation Accuracy: {}%\".format(val_acc*100)\n",
    "test_acc = compute_acc(model, data[\"data_test\"], data[\"labels_test\"])\n",
    "print \"Testing Accuracy: {}%\".format(test_acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "loss_hist_ = loss_hist[1::100] # sparse the curve a bit\n",
    "plt.plot(loss_hist_, '-o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(train_acc_hist, '-o', label='Training')\n",
    "plt.plot(val_acc_hist, '-o', label='Validation')\n",
    "plt.plot([0.5] * len(val_acc_hist), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Optimizers\n",
    "There are several more advanced optimizers than vanilla SGD, you will implement three more sophisticated and widely-used methods in this section. Please complete the TODOs in the optim.py under lib directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD + Momentum\n",
    "The update rule of SGD plus momentum is as shown below: <br\\ >\n",
    "\\begin{equation}\n",
    "v_t: velocity \\\\\n",
    "\\gamma: momentum \\\\\n",
    "\\eta: learning\\ rate \\\\\n",
    "v_t = \\gamma v_{t-1} + \\eta \\nabla_{\\theta}J(\\theta) \\\\\n",
    "\\theta = \\theta - v_t\n",
    "\\end{equation}\n",
    "Complete the SGDM() function in optim.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SGD with momentum\n",
    "model = TinyNet()\n",
    "loss_f = cross_entropy()\n",
    "optimizer = SGD(model.net, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test the implementation of SGD with Momentum\n",
    "N, D = 4, 5\n",
    "test_sgd = sequential(fc(N, D, name=\"sgd_fc\"))\n",
    "\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "test_sgd.layers[0].params = {\"sgd_fc_w\": w}\n",
    "test_sgd.layers[0].grads = {\"sgd_fc_w\": dw}\n",
    "\n",
    "test_sgd_momentum = SGDM(test_sgd, 1e-3, 0.9)\n",
    "test_sgd_momentum.velocity = {\"sgd_fc_w\": v}\n",
    "test_sgd_momentum.step()\n",
    "\n",
    "updated_w = test_sgd.layers[0].params[\"sgd_fc_w\"]\n",
    "velocity = test_sgd_momentum.velocity[\"sgd_fc_w\"]\n",
    "\n",
    "expected_updated_w = np.asarray([\n",
    "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
    "expected_velocity = np.asarray([\n",
    "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
    "\n",
    "print 'updated_w error: ', rel_error(updated_w, expected_updated_w)\n",
    "print 'velocity error: ', rel_error(expected_velocity, velocity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code block to train a multi-layer fully connected network with both SGD and SGD plus Momentum. The network trained with SGDM optimizer should converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Arrange a small data\n",
    "num_train = 4000\n",
    "small_data_dict = {\n",
    "    \"data_train\": (data[\"data_train\"][:num_train], data[\"labels_train\"][:num_train]),\n",
    "    \"data_val\": (data[\"data_val\"], data[\"labels_val\"]),\n",
    "    \"data_test\": (data[\"data_test\"], data[\"labels_test\"])\n",
    "}\n",
    "\n",
    "model_sgd      = FullyConnectedNetwork()\n",
    "model_sgdm     = FullyConnectedNetwork()\n",
    "loss_f_sgd     = cross_entropy()\n",
    "loss_f_sgdm    = cross_entropy()\n",
    "optimizer_sgd  = SGD(model_sgd.net, 1e-2)\n",
    "optimizer_sgdm = SGDM(model_sgdm.net, 1e-2, 0.9)\n",
    "\n",
    "print \"Training with Vanilla SGD...\"\n",
    "results_sgd = train_net(small_data_dict, model_sgd, loss_f_sgd, optimizer_sgd, batch_size=100, \n",
    "                        max_epochs=5, show_every=100, verbose=True)\n",
    "\n",
    "print \"\\nTraining with SGD plus Momentum...\"\n",
    "results_sgdm = train_net(small_data_dict, model_sgdm, loss_f_sgdm, optimizer_sgdm, batch_size=100, \n",
    "                         max_epochs=5, show_every=100, verbose=True)\n",
    "\n",
    "opt_params_sgd,  loss_hist_sgd,  train_acc_hist_sgd,  val_acc_hist_sgd  = results_sgd\n",
    "opt_params_sgdm, loss_hist_sgdm, train_acc_hist_sgdm, val_acc_hist_sgdm = results_sgdm\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgd, 'o', label=\"Vanilla SGD\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgd, '-o', label=\"Vanilla SGD\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgd, '-o', label=\"Vanilla SGD\")\n",
    "         \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgdm, 'o', label=\"SGD with Momentum\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgdm, '-o', label=\"SGD with Momentum\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgdm, '-o', label=\"SGD with Momentum\")\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp\n",
    "The update rule of RMSProp is as shown below: <br\\ >\n",
    "\\begin{equation}\n",
    "\\gamma: decay\\ rate \\\\\n",
    "\\epsilon: small\\ number \\\\\n",
    "g_t^2: squared\\ gradients \\\\\n",
    "\\eta: learning\\ rate \\\\\n",
    "E[g^2]_t: decaying\\ average\\ of\\ past\\ squared\\ gradients\\ at\\ update\\ step\\ t \\\\\n",
    "E[g^2]_t = \\gamma E[g^2]_{t-1} + (1-\\gamma)g_t^2 \\\\\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t+\\epsilon}}\n",
    "\\end{equation}\n",
    "Complete the RMSProp() function in optim.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test RMSProp implementation; you should see errors less than 1e-7\n",
    "N, D = 4, 5\n",
    "test_rms = sequential(fc(N, D, name=\"rms_fc\"))\n",
    "\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "test_rms.layers[0].params = {\"rms_fc_w\": w}\n",
    "test_rms.layers[0].grads = {\"rms_fc_w\": dw}\n",
    "\n",
    "opt_rms = RMSProp(test_rms, 1e-2, 0.99)\n",
    "opt_rms.cache = {\"rms_fc_w\": cache}\n",
    "opt_rms.step()\n",
    "\n",
    "updated_w = test_rms.layers[0].params[\"rms_fc_w\"]\n",
    "cache = opt_rms.cache[\"rms_fc_w\"]\n",
    "\n",
    "expected_updated_w = np.asarray([\n",
    "  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n",
    "  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n",
    "  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n",
    "  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n",
    "expected_cache = np.asarray([\n",
    "  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n",
    "  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n",
    "  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n",
    "  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n",
    "\n",
    "print 'updated_w error: ', rel_error(expected_updated_w, updated_w)\n",
    "print 'cache error: ', rel_error(expected_cache, opt_rms.cache[\"rms_fc_w\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "The update rule of Adam is as shown below: <br\\ >\n",
    "\\begin{equation}\n",
    "g_t: gradients\\ at\\ update\\ step\\ t \\\\\n",
    "m_t = \\beta_1m_{t-1} + (1-\\beta_1)g_t \\\\\n",
    "v_t = \\beta_2v_{t-1} + (1-\\beta_1)g_t^2 \\\\\n",
    "\\hat{m_t}: bias\\ corrected\\ m_t \\\\\n",
    "\\hat{v_t}: bias\\ corrected\\ v_t \\\\\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v_t}}+\\epsilon}\n",
    "\\end{equation}\n",
    "Complete the Adam() function in optim.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test Adam implementation; you should see errors around 1e-7 or less\n",
    "N, D = 4, 5\n",
    "test_adam = sequential(fc(N, D, name=\"adam_fc\"))\n",
    "\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
    "\n",
    "test_adam.layers[0].params = {\"adam_fc_w\": w}\n",
    "test_adam.layers[0].grads = {\"adam_fc_w\": dw}\n",
    "\n",
    "opt_adam = Adam(test_adam, 1e-2, 0.9, 0.999, t=5)\n",
    "opt_adam.mt = {\"adam_fc_w\": m}\n",
    "opt_adam.vt = {\"adam_fc_w\": v}\n",
    "opt_adam.step()\n",
    "\n",
    "updated_w = test_adam.layers[0].params[\"adam_fc_w\"]\n",
    "mt = opt_adam.mt[\"adam_fc_w\"]\n",
    "vt = opt_adam.vt[\"adam_fc_w\"]\n",
    "\n",
    "expected_updated_w = np.asarray([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
    "expected_m = np.asarray([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "print 'updated_w error: ', rel_error(expected_updated_w, updated_w)\n",
    "print 'mt error: ', rel_error(expected_m, mt)\n",
    "print 'vt error: ', rel_error(expected_v, vt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the optimizers\n",
    "Run the following code block to compare the plotted results among all the above optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_rms      = FullyConnectedNetwork()\n",
    "model_adam     = FullyConnectedNetwork()\n",
    "loss_f_rms     = cross_entropy()\n",
    "loss_f_adam    = cross_entropy()\n",
    "optimizer_rms  = RMSProp(model_rms.net, 5e-4)\n",
    "optimizer_adam = Adam(model_adam.net, 5e-4)\n",
    "\n",
    "print \"Training with RMSProp...\"\n",
    "results_rms = train_net(small_data_dict, model_rms, loss_f_rms, optimizer_rms, batch_size=100, \n",
    "                        max_epochs=5, show_every=100, verbose=True)\n",
    "\n",
    "print \"\\nTraining with Adam...\"\n",
    "results_adam = train_net(small_data_dict, model_adam, loss_f_adam, optimizer_adam, batch_size=100, \n",
    "                         max_epochs=5, show_every=100, verbose=True)\n",
    "\n",
    "opt_params_rms,  loss_hist_rms,  train_acc_hist_rms,  val_acc_hist_rms  = results_rms\n",
    "opt_params_adam, loss_hist_adam, train_acc_hist_adam, val_acc_hist_adam = results_adam\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgd, 'o', label=\"Vanilla SGD\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgd, '-o', label=\"Vanilla SGD\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgd, '-o', label=\"Vanilla SGD\")\n",
    "         \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgdm, 'o', label=\"SGD with Momentum\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgdm, '-o', label=\"SGD with Momentum\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgdm, '-o', label=\"SGD with Momentum\")\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_rms, 'o', label=\"RMSProp\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_rms, '-o', label=\"RMSProp\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_rms, '-o', label=\"RMSProp\")\n",
    "         \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_adam, 'o', label=\"Adam\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_adam, '-o', label=\"Adam\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_adam, '-o', label=\"Adam\")\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Network with Dropout\n",
    "Run the following code blocks to compare the results with and without dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train two identical nets, one with dropout and one without\n",
    "num_train = 500\n",
    "data_dict_500 = {\n",
    "    \"data_train\": (data[\"data_train\"][:num_train], data[\"labels_train\"][:num_train]),\n",
    "    \"data_val\": (data[\"data_val\"], data[\"labels_val\"]),\n",
    "    \"data_test\": (data[\"data_test\"], data[\"labels_test\"])\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "dropout_ps = [0, 0.25]  # you can try some dropout prob yourself\n",
    "\n",
    "results_dict = {}\n",
    "for dropout_p in dropout_ps:\n",
    "    results_dict[dropout_p] = {}\n",
    "\n",
    "for dropout_p in dropout_ps:\n",
    "    print \"Dropout =\", dropout_p\n",
    "    model = DropoutNetTest(dropout_p=dropout_p)\n",
    "    loss_f = cross_entropy()\n",
    "    optimizer = SGDM(model.net, 1e-4)\n",
    "    results = train_net(data_dict_500, model, loss_f, optimizer, batch_size=100, \n",
    "                        max_epochs=20, show_every=100, verbose=True)\n",
    "    opt_params, loss_hist, train_acc_hist, val_acc_hist = results\n",
    "    results_dict[dropout_p] = {\n",
    "        \"opt_params\": opt_params, \n",
    "        \"loss_hist\": loss_hist, \n",
    "        \"train_acc_hist\": train_acc_hist, \n",
    "        \"val_acc_hist\": val_acc_hist\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot train and validation accuracies of the two models\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "for dropout_p in dropout_ps:\n",
    "    curr_dict = results_dict[dropout_p]\n",
    "    train_accs.append(curr_dict[\"train_acc_hist\"][-1])\n",
    "    val_accs.append(curr_dict[\"val_acc_hist\"][-1])\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "for dropout_p in dropout_ps:\n",
    "    curr_dict = results_dict[dropout_p]\n",
    "    plt.plot(curr_dict[\"train_acc_hist\"], 'o', label='%.2f dropout' % dropout_p)\n",
    "plt.title('Train accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "  \n",
    "plt.subplot(3, 1, 2)\n",
    "for dropout_p in dropout_ps:\n",
    "    curr_dict = results_dict[dropout_p]\n",
    "    plt.plot(curr_dict[\"val_acc_hist\"], 'o', label='%.2f dropout' % dropout_p)\n",
    "plt.title('Val accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline Question: Describe what you observe from the above results and graphs\n",
    "#### Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Activation Functions\n",
    "In each of the activation function, use the given lambda function template to plot their corresponding curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "left, right = -10, 10\n",
    "X  = np.linspace(left, right, 100)\n",
    "XS = np.linspace(-5, 5, 10)\n",
    "lw = 4\n",
    "alpha = 0.1\n",
    "elu_alpha = 0.5\n",
    "selu_alpha = 1.6732\n",
    "selu_scale = 1.0507\n",
    "\n",
    "#########################\n",
    "####### YOUR CODE #######\n",
    "#########################\n",
    "sigmoid = lambda x: x\n",
    "leaky_relu = lambda x: x\n",
    "relu = lambda x: x\n",
    "elu = lambda x: x\n",
    "selu = lambda x: x\n",
    "tanh = lambda x: x\n",
    "#########################\n",
    "### END OF YOUR CODE ####\n",
    "#########################\n",
    "\n",
    "activations = {\n",
    "    \"Sigmoid\": sigmoid,\n",
    "    \"LeakyReLU\": leaky_relu,\n",
    "    \"ReLU\": relu,\n",
    "    \"ELU\": elu,\n",
    "    \"SeLU\": selu,\n",
    "    \"Tanh\": tanh\n",
    "}\n",
    "\n",
    "# Ground Truth activations\n",
    "GT_Act = {\n",
    "    \"Sigmoid\": [0.00669285092428, 0.0200575365379, 0.0585369028744, 0.158869104881, 0.364576440742, \n",
    "                0.635423559258, 0.841130895119, 0.941463097126, 0.979942463462, 0.993307149076],\n",
    "    \"LeakyReLU\": [-0.5, -0.388888888889, -0.277777777778, -0.166666666667, -0.0555555555556, \n",
    "                  0.555555555556, 1.66666666667, 2.77777777778, 3.88888888889, 5.0],\n",
    "    \"ReLU\": [-0.0, -0.0, -0.0, -0.0, -0.0, 0.555555555556, 1.66666666667, 2.77777777778, 3.88888888889, 5.0],\n",
    "    \"ELU\": [-0.4966310265, -0.489765962143, -0.468911737989, -0.405562198581, -0.213123289631, \n",
    "            0.555555555556, 1.66666666667, 2.77777777778, 3.88888888889, 5.0],\n",
    "    \"SeLU\": [-1.74618571868, -1.72204772347, -1.64872296837, -1.42598202974, -0.749354802287, \n",
    "             0.583722222222, 1.75116666667, 2.91861111111, 4.08605555556, 5.2535],\n",
    "    \"Tanh\": [-0.999909204263, -0.999162466631, -0.992297935288, -0.931109608668, -0.504672397722, \n",
    "             0.504672397722, 0.931109608668, 0.992297935288, 0.999162466631, 0.999909204263]\n",
    "} \n",
    "\n",
    "for label in activations:\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.plot(X, activations[label](X), color='darkorchid', lw=lw, label=label)\n",
    "    assert rel_error(activations[label](XS), GT_Act[label]) < 1e-9, \\\n",
    "           \"Your implementation of {} might be wrong\".format(label)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.axhline(0, color='black')\n",
    "    ax.axvline(0, color='black')\n",
    "    ax.set_title('{}'.format(label), fontsize=14)\n",
    "    plt.xlabel(r\"X\")\n",
    "    plt.ylabel(r\"Y\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Phew! You're done for problem 1 now, but 3 more to go... LOL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
